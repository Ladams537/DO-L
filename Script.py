# -*- coding: utf-8 -*-
"""Problem_14_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zV_we5q3H1ESTptTA3Pg-4caRtCi5Aw8

**Problem 14.2** For a deterministic optimization problem of your choice, implement the consensus+innovation, EXTRA and gradient-tracking algorithms and show that distributed gradient descent exhibits a bias, while EXTRA and gradient-tracking converge exactly. How do these findings change with the choice of the step-size $\mu$?


**Solution.** We generate quadratic problems in a manner similar to **Problem 5.2** to demonstrate the benefit of employing bias-corrected algorithms. An advantage of quadratic problems is that we will be possible to compute exact solutions in closed form. This simplifies the problem of evaluating the error.

We generate local data sets $\{ h_{k, n}, \gamma_{k, n} \}_{n=1}^N$ following the statistical model of Example 5.4. To exacerbate the local heterogeneity in the data sets, we use variable local models $w_k$. We begin by importing some standard packages which will be useful throughout this exercise:
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
# %matplotlib inline

np.random.seed(0)

"""We begin by setting the variance parameters $\sigma_w^2, \sigma_h^2, \sigma_v^2$, network size $K$, sample size $N$ and dimension $M$. Then generate a realization of the weight vector $w$ by sampling once from the normal distribution $\mathcal{N}(0, \sigma_w^2 I_M)$. For each agent $k$, we sample $N$ times from $\mathcal{N}(0, \sigma_h^2 I_M)$ to generate $\{ h_{k, n} \}_{n=1}^N$, and $N$ times from $\mathcal{N}(0, \sigma_v^2)$ to generate $\{ v_{k, n} \}_{n=1}^N$. $\{ \gamma_{k, n} \}_{n=1}^N$ are then generated according to the linear model (5.42). We store data sets in matrices for compact coding."""

sigma_h_squared = 1
sigma_v_squared = 1
sigma_w_squared = 1
sigma_w_variation_squared = 1

M = 2
K = 4
N = 15

rho = 0

w = np.random.multivariate_normal(np.zeros(M), np.square(sigma_w_squared)*np.eye(M), K).T

h = np.zeros((M, N, K))
v = np.zeros((N, K))
gamma = np.zeros((N, K))
for k in range(K):
    h[:, :, k] = np.random.multivariate_normal(np.zeros(M), sigma_h_squared*np.eye(M), N).T
    v[:, k] = np.random.normal(0, sigma_v_squared, N).T
    gamma[:, k] = np.matmul(h[:, :, k].T, w[:, k]) + v[:, k]

"""We now compute the locally and globally optimal models using the same calculations as in Problem 5.2."""

H_k = np.zeros((M, M, K))
d_k = np.zeros((M, K))

H = np.zeros((M, M))
d = np.zeros(M)

w_k_star = np.zeros((M, K))

for k in range(K):
    for n in range(N):
        H_k[:, :, k] += np.outer(h[:, n, k], h[:, n, k])
        d_k[:, k] += gamma[n, k]*h[:, n, k]
    w_k_star[:, k] = np.linalg.solve(H_k[:, :, k] + rho*np.eye(M), d_k[:, k])

    H += H_k[:, :, k]
    d += d_k[:, k]

w_star = np.linalg.solve(H + rho*np.eye(M), d)

"""Now, we generate a graph using the structure of Problem 13.2."""

p_edge = 0.1

lambda_2 = 1
while lambda_2 > 0.99999999:
    C = np.eye(K)
    for k in range(K):
        for l in range(k+1, K):
            connected = np.random.binomial(1, p_edge)
            if connected == 1:
                C[l, k] = 1
                C[k, l] = 1

    n = C@np.ones((K,))

    A = np.zeros((K, K))
    for k in range(K):
        for l in range(k+1, K):
            if C[k, l] == 1:
                A[k, l] = np.true_divide(1, np.max([n[k], n[l]]))
                A[l, k] = A[k, l]

    degrees = A@np.ones((K,))
    for k in range(K):
        A[k, k] = 1 - degrees[k]

    eigs = np.linalg.eigvalsh(A)
    lambda_2 = eigs[-2]

"""We are not ready to implement the distributed gradient descent, EXTRA and NEXT algorithm."""

iterations = 1000
experiments = 1
mu = 0.01

w_ci = np.zeros((M, K, iterations, experiments))
psi_ci = np.zeros((M, K, iterations, experiments))
w_extra = np.zeros((M, K, iterations, experiments))
w_tracking = np.zeros((M, K, iterations, experiments))

psi_extra = np.zeros((M, K, iterations, experiments))
g_tracking = np.zeros((M, K, iterations, experiments))

error_ci = np.zeros((K, iterations, experiments))
error_extra = np.zeros((K, iterations, experiments))
error_tracking = np.zeros((K, iterations, experiments))

for run in range(experiments):

    # For all algorithms, we implement the very first iteration by hand. This is particular important for EXTRA and Gradient-Tracking, since care needs to be taken with the initialization.
    # Consensus + Innovations
    for k in range(K):
        for l in range(K):
            w_ci[:, k, 1, run] += A[l, k] * w_ci[:, l, 0, run]
    for k in range(K):
        w_ci[:, k, 1, run] += - np.true_divide(mu, N) * rho * w_ci[:, k, 0, run] + np.true_divide(mu, N) * (d_k[:, k] - np.dot(H_k[:, :, k].T, w_ci[:, k, 0, run]))

        error_ci[k, 0, run] = np.square(np.linalg.norm(w_ci[:, k, 0, run] - w_star))
        error_ci[k, 1, run] = np.square(np.linalg.norm(w_ci[:, k, 1, run] - w_star))

    # EXTRA
    # The dual variables of EXTRA are initialized at zero, which means that the very first iteration is just a consensus+innovations step.
    for k in range(K):
        for l in range(K):
            w_extra[:, k, 1, run] += A[l, k]*w_extra[:, l, 0, run]
    for k in range(K):
        psi_extra[:, k, 1, run] = w_extra[:, k, 1, run] - np.true_divide(mu, N) * rho * w_extra[:, k, 0, run] + np.true_divide(mu, N) * (d_k[:, k] - np.dot(H_k[:, :, k].T, w_extra[:, k, 0, run]))
        w_extra[:, k, 1, run] = psi_extra[:, k, 1, run]
        error_extra[k, 0, run] = np.square(np.linalg.norm(w_extra[:, k, 0, run] - w_star))
        error_extra[k, 1, run] = np.square(np.linalg.norm(w_extra[:, k, 1, run] - w_star))

    # NEXT
    # The gradient estimates are initialized to the local gradients.
    for k in range(K):
        g_tracking[:, k, 0, run] = np.true_divide(1, N) * rho * w_tracking[:, k, 0, run] - np.true_divide(1, N) * (d_k[:, k] - np.dot(H_k[:, :, k].T, w_tracking[:, k, 0, run]))
    for k in range(K):
        for l in range(K):
            w_tracking[:, k, 1, run] += A[l, k]*w_tracking[:, l, 0, run]
            g_tracking[:, k, 1, run] += A[l, k]*g_tracking[:, l, 0, run]

    for k in range(K):
        w_tracking[:, k, 1, run] += - mu * g_tracking[:, k, 0, run]

        new_gradient = np.true_divide(1, N) * rho * w_tracking[:, k, 1, run] - np.true_divide(1, N) * (d_k[:, k] - np.dot(H_k[:, :, k].T, w_tracking[:, k, 1, run]))
        old_gradient = np.true_divide(1, N) * rho * w_tracking[:, k, 0, run] - np.true_divide(1, N) * (d_k[:, k] - np.dot(H_k[:, :, k].T, w_tracking[:, k, 0, run]))

        g_tracking[:, k, 1, run] += new_gradient - old_gradient

        error_tracking[k, 0, run] = np.square(np.linalg.norm(w_tracking[:, k, 0, run] - w_star))
        error_tracking[k, 1, run] = np.square(np.linalg.norm(w_tracking[:, k, 1, run] - w_star))

    for i in range(2, iterations):
        # Consensus + innovations
        for k in range(K):
            for l in range(K):
                w_ci[:, k, i, run] += A[l, k] * w_ci[:, l, i-1, run]
        for k in range(K):
            w_ci[:, k, i, run] += - np.true_divide(mu, N) * rho * w_ci[:, k, i-1, run] + np.true_divide(mu, N) * (d_k[:, k] - np.dot(H_k[:, :, k].T, w_ci[:, k, i-1, run]))

            error_ci[k, i, run] = np.square(np.linalg.norm(w_ci[:, k, i, run] - w_star))

        # EXTRA
        for k in range(K):
            for l in range(K):
                w_extra[:, k, i, run] += A[l, k] * w_extra[:, l, i-1, run]

        for k in range(K):
            psi_extra[:, k, i, run] = w_extra[:, k, i, run]
            psi_extra[:, k, i, run] += -np.true_divide(mu, N) * rho * w_extra[:, k, i-1, run] + np.true_divide(mu, N) * (d_k[:, k] - np.dot(H_k[:, :, k].T, w_extra[:, k, i-1, run]))
            w_extra[:, k, i, run] += psi_extra[:, k, i, run] - psi_extra[:, k, i-1, run]
            error_extra[k, i, run] = np.square(np.linalg.norm(w_extra[:, k, i, run] - w_star))

        # NEXT
        for k in range(K):
            for l in range(K):
                w_tracking[:, k, i, run] += A[l, k]*w_tracking[:, l, i-1, run]
                g_tracking[:, k, i, run] += A[l, k]*g_tracking[:, l, i-1, run]
        for k in range(K):
            w_tracking[:, k, i, run] += - mu * g_tracking[:, k, i-1, run]

            new_gradient = np.true_divide(1, N) * rho * w_tracking[:, k, i, run] - np.true_divide(1, N) * (d_k[:, k] - np.dot(H_k[:, :, k].T, w_tracking[:, k, i, run]))
            old_gradient = np.true_divide(1, N) * rho * w_tracking[:, k, i-1, run] - np.true_divide(1, N) * (d_k[:, k] - np.dot(H_k[:, :, k].T, w_tracking[:, k, i-1, run]))

            g_tracking[:, k, i, run] += new_gradient - old_gradient

            error_tracking[k, i, run] = np.square(np.linalg.norm(w_tracking[:, k, i, run] - w_star))

"""Finally, we generate the learning curves and plot."""

learning_curve_ci = np.mean(np.mean(error_ci, axis=2), axis=0)
learning_curve_extra = np.mean(np.mean(error_extra, axis=2), axis=0)
learning_curve_tracking = np.mean(np.mean(error_tracking, axis=2), axis=0)
plt.figure()
plt.semilogy(range(iterations), learning_curve_ci, linewidth=2, label='Consensus+innovations')
plt.semilogy(range(iterations), learning_curve_extra, linewidth=2, label='EXTRA')
plt.semilogy(range(iterations), learning_curve_tracking, ':', linewidth=2, label='Gradient-tracking')
plt.xlabel('Iteration',fontsize=12,fontname='times new roman')
plt.ylabel('MSD in dB',fontsize= 12,fontname='times new roman' )
plt.xlim(0,iterations)
plt.legend()
plt.grid()
plt.show()

"""We observe that both EXTRA and gradient-tracking converge exactly, while distributed gradient descent exhibits a bias. Reducing the step-size by a factor of $10$ reduces the bias by a factor of $100$, but is also associated with a reduction in the rate of convergence for all three algorithms."""